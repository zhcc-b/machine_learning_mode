# -*- coding: utf-8 -*-
"""Appendix:

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G9U5EnXo3d-gweSsvwLc30Pvm3DtnKPE
"""

!apt-get update &> /dev/null
!sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-plain-generic &> /dev/null
!jupyter nbconvert --to pdf /content/drive/MyDrive/'Colab Notebooks'/'Appendix:' &> /dev/null

"""# Function Loading

"""

"""
This Python file provides some useful code for reading the training file
"clean_quercus.csv". You may adapt this code as you see fit. However,
keep in mind that the code provided does only basic feature transformations
to build a rudimentary kNN model in sklearn. Not all features are considered
in this code, and you should consider those features! Use this code
where appropriate, but don't stop here!
"""
import matplotlib.pyplot as plt
import re
import pandas as pd
import numpy as np
from sklearn.neighbors import KNeighborsClassifier 
from numpy import savetxt
from sklearn.model_selection import train_test_split
random_state = 42

def to_numeric(s):
    """Converts string `s` to a float.

    Invalid strings and NaN values will be converted to float('nan').
    """

    if isinstance(s, str):
        s = s.replace(",", '')
        s = pd.to_numeric(s, errors="coerce")
    return float(s)

def get_number_list(s):
    """Get a list of integers contained in string `s`
    """
    return [int(n) for n in re.findall("(\d+)", str(s))]

def get_number_list_clean(s):
    """Return a clean list of numbers contained in `s`.

    Additional cleaning includes removing numbers that are not of interest 
    and standardizing return list size.
    """
    s = s.replace("3-D", '')
    s = s.replace("14-dimensional", '')
    n_list = get_number_list(s)
    n_list += [-1]*(5-len(n_list))
    return n_list

def get_number(s):
    """Get the first number contained in string `s`.

    If `s` does not contain any numbers, return -1.
    """
    n_list = get_number_list(s)
    return n_list[0] if len(n_list) >= 1 else -1

def find_quote_at_rank(l, i):
    """Return the quote at a certain rank in list `l`.

    Quotes are indexed starting at 1 as ordered in the survey.

    If quote is not present in `l`, return -1.
    """
    return l.index(i) + 1 if i in l else -1

def cat_in_s(s, cat):
    """Return if a category is present in string `s` as an binary integer.
    """
    return int(cat in s) if not pd.isna(s) else 0

"""# Data set modification"""

np.seterr(over='ignore')

def get_data(flag):

  # Get data from drive
  url='https://drive.google.com/file/d/1dUF6VQ0kjAhQ0vGsSGSLNMtjidYs7NwR/view?usp=sharing'
  url='https://drive.google.com/uc?id=' + url.split('/')[-2]
  df = pd.read_csv(url)

  # Drop missing value's responses.
  # if flag:
  df = df.dropna()
  df = df.drop('user_id', axis=1)

  # Change sell price to float 
  df["q_sell"] = df["q_sell"].apply(to_numeric)
  df["q_temperature"] = df["q_temperature"].apply(to_numeric)

  # Change scary-lv to int
  df["q_scary"] = df["q_scary"].apply(get_number)
  df["q_dream"] = df["q_dream"].apply(get_number)
  df["q_desktop"] = df["q_desktop"].apply(get_number)

  # Create quote rank categories to int
  df["q_quote"] = df["q_quote"].apply(get_number_list_clean)

  # Divide 5 quotes into 5 cols, and assign user's rank to the quote.
  # df["q_quote"] becomes df["rank_1"], ... , df["rank_5"]
  ranks = []
  for i in range(1,6):
      col_name = f"rank_{i}"
      ranks.append(col_name)
      df[col_name] = df["q_quote"].apply(lambda l: find_quote_at_rank(l, i))
  del df["q_quote"]

  if flag:
    # Drop rows where quote rank is blank/-1.
    for i in range(1,6):
        col_name = f"rank_{i}"
        df = df[df[f"rank_{i}"] != -1]

  # Separate scary-lv into categories, use 0 or 1 to indicate
  scary_cat = []

  for col in ["q_scary"]:
      indicators = pd.get_dummies(df[col], prefix=col)
      scary_cat.extend(indicators.columns)
      df = pd.concat([df, indicators], axis=1)
      del df[col]

  # Separate ranks into categories, use 0 or 1 to indicate
  ranks_cat = []
  for col in ranks:
      indicators = pd.get_dummies(df[col], prefix=col)
      ranks_cat.extend(indicators.columns)
      df = pd.concat([df, indicators], axis=1)
      del df[col]

  # Create multi-category indicators
  for cat in ["Parents", "Siblings", "Friends", "Teacher"]:
      df[f"q_remind_{cat}"] = df["q_remind"].apply(lambda s: cat_in_s(s, cat))
  del df["q_remind"]

  for cat in ["People", "Cars", "Cats", "Fireworks", "Explosions"]:
      df[f"q_better_{cat}"] = df["q_better"].apply(lambda s: cat_in_s(s, cat))
  del df["q_better"]

  # There is a bias and will be removed
  print("Bias Romoved (Temp): ", df.at[174, "q_temperature"], "\n")
  df = df.drop([174])


  # Droped these 2 bias term reduced Avg from 724.2 to 19.87

  print("Bias Romoved (Temp): ", df.at[185, "q_sell"], "\n")
  df = df.drop([185])

  print("Bias Romoved (Sell): ", df.at[200, "q_sell"], "\n")
  df = df.drop([200])

  print("Bias Romoved (Sell): ", df.at[319, "q_sell"], "\n")
  df = df.drop([319])

  print("Bias Romoved (Sell): ", df.at[542, "q_sell"], "\n")
  df = df.drop([542])

  if flag:
    return [df, scary_cat, ranks_cat]
  return df

"""# Data Visiulization"""

df = get_data(1)
df.to_csv("data_set_modify1.csv")
df = get_data(10)
df.to_csv("data_set_modify0.csv")

"""# Normalization of Trainning, Validation & Test set (For Lib Models)

"""

data = get_data(1)

df = data[0]
scary_cat = data[1]
ranks_cat = data[2]

df.to_csv("old.csv")

# Randomize Data
df = df.sample(frac=1)

# t: target, X: data
t = df["label"]
X = np.array(df.drop(['label','q_story'], axis=1))

sample_size = df.shape[0]
vt_sample_size = round(sample_size*0.9)
test_size = round(sample_size*0.1)

# Split into X_train, X_valid, X_test and corresponding target
X_tv, X_test, t_tv, t_test = train_test_split(X, t, test_size= test_size/sample_size, random_state=0)
X_train, X_valid, t_train, t_valid= train_test_split(X_tv, t_tv, test_size=round(0.25*vt_sample_size)/vt_sample_size, random_state=0)

# Calculate mean and sd
mean_3 = X_train[:, 3].mean(axis=0)
std_3 = X_train[:, 3].std(axis=0)

mean_4 = X_train[:, 4].mean(axis=0)
std_4 = X_train[:, 4].std(axis=0)

# Normalize
X_train_norm = X_train.copy()
X_valid_norm = X_valid.copy()
X_test_norm = X_test.copy()

X_train_norm[:, 3] = (X_train[:, 3] - mean_3) / std_3
X_valid_norm[:, 3] = (X_valid[:, 3] - mean_3) / std_3
X_test_norm[:, 3] = (X_test[:, 3] - mean_3) / std_3

X_train_norm[:, 4] = (X_train[:, 4] - mean_4) / std_4
X_valid_norm[:, 4] = (X_valid[:, 4] - mean_4) / std_4
X_test_norm[:, 4] = (X_test[:, 4] - mean_4) / std_4

print(X_train_norm[:, 3:5].mean(axis=0)) # Should be all very close to 0
print(X_train_norm[:, 3:5].std(axis=0))  # Should be all very close to 1
print(X_valid_norm[:, 3:5].mean(axis=0)) # Should be close to 0
print(X_valid_norm[:, 3:5].std(axis=0))  # Should be close to 1
print(X_test_norm[:, 3:5].mean(axis=0)) # Should be close to 0
print(X_test_norm[:, 3:5].std(axis=0))  # Should be close to 1

print(X_train_norm.shape)
print(X_valid_norm.shape)
print(X_test_norm.shape)

"""# KNN"""

knn = df[scary_cat + ranks_cat + ["q_sell", "label"]]
x = knn.drop("label", axis=1).values
y = pd.get_dummies(knn["label"].values)
clf = KNeighborsClassifier(n_neighbors=3)

X_train_knn = np.concatenate((X_train_norm, X_valid_norm), axis=0)
t_train_knn = np.concatenate((t_train, t_valid), axis=0)

clf.fit(X_train_knn, t_train_knn)
train_acc = clf.score(X_train_norm, t_train)
test_acc = clf.score(X_test_norm, t_test)
print(f"{type(clf).__name__} train acc: {train_acc}")
print(f"{type(clf).__name__} test acc: {test_acc}")

"""# Byes Classfier"""

from sklearn.naive_bayes import GaussianNB
vocab = [] 
data = df["q_story"].to_numpy()
for i in data:
  if isinstance(i , str):
    for j in i.split():
      if j not in vocab:
        vocab.append(j)

print("Vocabulary Size: ", len(vocab))

t = df["label"].to_numpy()
def make_bow(data, vocab):
    X = np.zeros([len(data), len(vocab)])
    for i in range(len(data)):
      if isinstance(data[i] , str):
        for j in data[i].split():
          X[i, vocab.index(j)] = 1
    return X

X_train_bc, t_train_bc = make_bow(data[:400], vocab), t[:400]
X_test_bc, t_test_bc = make_bow(data[400:], vocab), t[400:]
gnb = GaussianNB()
gnb.fit(X_train_bc, t_train_bc)
train_acc = gnb.score(X_train_bc, t_train_bc)
test_acc = gnb.score(X_test_bc, t_test_bc)
print(f"{type(gnb).__name__} train acc: {train_acc}")
print(f"{type(gnb).__name__} test acc: {test_acc}")

"""# Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
tree = DecisionTreeClassifier(criterion="entropy", max_depth=4)

X_train_tree = np.concatenate((X_train_norm, X_valid_norm), axis=0)
t_train_tree = np.concatenate((t_train, t_valid), axis=0)

tree.fit(X_train_tree,t_train_tree)
print("Tree model score:")
print("Training:",tree.score(X_train_tree, t_train_tree))
print("Test",tree.score(X_test_norm, t_test))

"""#Decision Forest"""

from sklearn.ensemble import RandomForestClassifier

n_estimators_choices = [1, 10, 50, 100, 1000, 2000]

for i in n_estimators_choices:
  forest = RandomForestClassifier(criterion="gini",
                                max_depth=10,
                                min_samples_split=16,
                                n_estimators=i)
  forest.fit(X_train, t_train)
  print(i)
  print("Training Accuracy", forest.score(X_train, t_train))
  print("Test Accuracy", forest.score(X_test_norm, t_test))

"""# Logistic Regression"""

from sklearn.linear_model import LogisticRegression

X_train_log = np.concatenate((X_train_norm, X_valid_norm), axis=0)
t_train_log = np.concatenate((t_train, t_valid), axis=0)

lgs = LogisticRegression(max_iter = 1000000000).fit(X_train_norm, t_train)
print("Logistic model score:")
print("Training:",lgs.score(X_train_log, t_train_log))
print("Test",lgs.score(X_test_norm, t_test))

"""#Multilayer Perceptron"""

from sklearn.neural_network import MLPClassifier

X_train_MLP = np.concatenate((X_train_norm, X_valid_norm), axis=0)
t_train_MLP = np.concatenate((t_train, t_valid), axis=0)

mlp =  MLPClassifier(max_iter = 10000).fit(X_train_MLP,t_train_MLP)
print("MLP model score:")
print("Training:",mlp.score(X_train_MLP, t_train_MLP))
print("Test",mlp.score(X_test_norm, t_test))

"""# My Model

Added bias term and treat turn y into one hot functions. Removed rows contains unfilled data in training and validation set.
"""

df = get_data(0)
df.to_csv("new.csv")

# Randomize Data
df = df.sample(frac=1)

# t: target, X: data
t = df["label"]
X = np.array(df.drop(['label','q_story'],axis=1))

# Add bias term to X
ones = np.ones((X.shape[0], 1))
X = np.concatenate((ones, X), axis = 1)

# Change y to one-hot function
values = np.max(t) + 1
t = np.eye(values)[t]
t = t[:,1:]

sample_size = df.shape[0]
vt_sample_size = round(sample_size*0.9)
test_size = round(sample_size*0.1)

# Split into X_train, X_valid, X_test and corresponding target
X_tv, X_test, t_tv, t_test = train_test_split(X, t, test_size= test_size/sample_size, random_state=0)
X_train, X_valid, t_train, t_valid= train_test_split(X_tv, t_tv, test_size=round(0.25*vt_sample_size)/vt_sample_size, random_state=0)

# -------------- Remove rows contain -1/blank from tranning set --------------

# By doing so, the weights for data with missing information will not be trained
# since we do not want to train our model by using invalid data. Instead, after
# the model is trained, we add 0 as weights for the columns represents the 
# missing information.
X_train = np.concatenate((X_train, t_train), axis = 1)

col_index = [15, 21, 27, 33, 39]
row_index = []
for col in col_index:
  lst = np.where(X_train[:, col] == 1)[0].tolist()
  for ele in lst:
    if ele not in row_index:
      row_index.append(ele)

X_train = np.delete(X_train, tuple(row_index), axis = 0)
X_train = np.delete(X_train, tuple(col_index), axis = 1)
t_train = X_train[:, -3:]
X_train = X_train[:, :-3]
# ----------------------------------------------------------------------------

# Keep the same size as X_train such that can be used for generating graphs in 
# SGD later. After the model is trained, we add 0 weights back to represents
# missing informations and then use X_test.
X_valid = np.delete(X_valid, tuple(col_index), axis = 1)


# Calculate mean and sd
mean_3 = X_train[:, 3].mean(axis=0)
std_3 = X_train[:, 3].std(axis=0)

mean_4 = X_train[:, 4].mean(axis=0)
std_4 = X_train[:, 4].std(axis=0)

# Normalize
X_train_norm = X_train.copy()
X_valid_norm = X_valid.copy()
X_test_norm = X_test.copy()

X_train_norm[:, 3] = (X_train[:, 3] - mean_3) / std_3
X_valid_norm[:, 3] = (X_valid[:, 3] - mean_3) / std_3
X_test_norm[:, 3] = (X_test[:, 3] - mean_3) / std_3

X_train_norm[:, 4] = (X_train[:, 4] - mean_4) / std_4
X_valid_norm[:, 4] = (X_valid[:, 4] - mean_4) / std_4
X_test_norm[:, 4] = (X_test[:, 4] - mean_4) / std_4

print(X_train_norm[:, 3:5].mean(axis=0)) # Should be all very close to 0
print(X_train_norm[:, 3:5].std(axis=0))  # Should be all very close to 1
print(X_valid_norm[:, 3:5].mean(axis=0)) # Should be close to 0
print(X_valid_norm[:, 3:5].std(axis=0))  # Should be close to 1
print(X_test_norm[:, 3:5].mean(axis=0)) # Should be close to 0
print(X_test_norm[:, 3:5].std(axis=0))  # Should be close to 1

def sigmoid(x):
    """
    Apply the sigmoid activation to a numpy matrix `x` of any shape.
    """
    return 1 / (1 + np.exp(-x))

def pred(w, X):
    """
    Compute the prediction made by a logistic regression model with weights `w`
    on the data set with input data matrix `X`. Recall that N is the number of 
    samples and D is the number of features. The +1 accounts for the bias term.

    Parameters:
        `weight` - a numpy array of shape (D+1)
        `X` - data matrix of shape (N, D+1)

    Returns: Prediction vector `y` of shape (N). Each value in `y` should be betwen 0 and 1.
    """

    z = np.dot(X, w)
    y = sigmoid(z)
    return y

def predt(w_lst,X):

    ans = []

    for i in range(len(w_lst)):
      ans.append(pred(w_lst[i],X))
  
    ans = np.array(ans).T

    # Return index of max value alone col (i.e. prob in this case)
    final = np.argmax(ans, axis = 1) + 1

    return final

def loss(w, X, t):
    """
    Compute the average cross-entropy loss of a logistic regression model
    with weights `w` on the data set with input data matrix `X` and
    targets `t`

    Parameters:
        `weight` - a numpy array of shape (D+1)
        `X` - data matrix of shape (N, D+1)
        `t` - target vector of shape (N)

    Returns: a scalar cross entropy loss value, computed using the numerically
             stable np.logaddexp function.
    """
    
    z=np.matmul(X,w)
    log1 = np.logaddexp(0,(-z))
    log2 = np.logaddexp(0,(z))

    return np.matmul(t,log1)-np.matmul((1-t),log2)

def accuracy(w, X, t, thres=0.5):
    """
    Compute the accuracy of a logistic regression model with weights `w`
    on the data set with input data matrix `X` and targets `t`

    If the logistic regression model prediction is y >= thres, then
    predict that t = 1. Otherwise, predict t = 0.
    (Note that this is an arbitrary decision that we are making, and
    it makes virtually no difference if we decide to predict t = 0 if 
    y == thres exactly, since the chance of y == thres is highly
    improbable.)

    Parameters:
        `weight` - a numpy array of shape (D+1)
        `X` - data matrix of shape (N, D+1)
        `t` - target vector of shape (N)
        `thres` - a value between 0 and 1

    Returns: accuracy value, between 0 and 1
    """
    y = pred(w, X)
    predictions = (y >= thres).astype(int)
    return np.mean(predictions == t)

def grad(w, X, t):
    '''
    Return gradient of the cost function at `w`. The cost function
    is the average cross entropy loss across the data set X and the
    target vector t.

    Parameters:
        `weight` - a current "guess" of what our weights should be,
                   a numpy array of shape (D+1)
        `X` - matrix of shape (N,D+1) of input features
        `t` - target y values of shape (N)

    Returns: gradient vector of shape (D+1)
    '''
    y = pred(w, X)

    return np.dot((y - t), X)/(X.shape[0])

def solve_via_gradient_descent(alpha=0.0002, niter=1000,
                               X_train=X_train, t_train=t_train,
                               X_valid=X_valid, t_valid=t_valid,
                               w_init=None, plot=True):
    '''
    Given `alpha` - the learning rate
          `niter` - the number of iterations of gradient descent to run
          `X_train` - the data matrix to use for training
          `t_train` - the target vector to use for training
          `X_valid` - the data matrix to use for validation
          `t_valid` - the target vector to use for validation
          `w_init` - the initial `w` vector (if `None`, use a vector of all zeros)
          `plot` - whether to track statistics and plot the training curve

    Solves for logistic regression weights via full batch gradient descent.
    Return weights after `niter` iterations.
    '''
    # initialize all the weights to zeros
    w1 = np.zeros(X_train.shape[1])
    w2 = np.zeros(X_train.shape[1])
    w3 = np.zeros(X_train.shape[1])
    w = np.array([w1, w2, w3])
  
    # we will track the loss and accuracy values at each iteration to record progress
    train_loss = [[],[],[]]
    valid_loss = [[],[],[]]
    train_acc = [[],[],[]]
    valid_acc = [[],[],[]]

    for i in range(niter):
      for iw in range(w.shape[0]):
        dw = grad(w[iw], X_train, t_train[:,iw])
        w[iw] = w[iw] - alpha * dw

        if plot:
            # Record the current training and validation loss values.
            train_loss[iw].append(loss(w[iw], X_train, t_train[:,iw]))
            valid_loss[iw].append(loss(w[iw], X_valid, t_valid[:,iw]))
            train_acc[iw].append(accuracy(w[iw], X_train, t_train[:,iw]))
            valid_acc[iw].append(accuracy(w[iw], X_valid, t_valid[:,iw]))

    if plot:
        for i in range(len(train_loss)):
          plt.title("Training Curve Showing Training and Validation Loss at each Iteration")
          plt.plot(train_loss[i], label=f"Training Loss {i}")
          plt.plot(valid_loss[i], label=f"Validation Loss {i}")
          plt.xlabel("Iterations")
          plt.ylabel("Loss")
          plt.show()

          plt.title("Training Curve Showing Training and Validation Accuracy at each Iteration")
          plt.plot(train_acc[i], label=f"Training Accuracy {i}")
          plt.plot(valid_acc[i], label=f"Validation Accuracy {i}")
          plt.xlabel("Iterations")
          plt.ylabel("Accuracy")
          plt.show()

          print("Final Training Loss:", train_loss[i][-1])
          print("Final Validation Loss:", valid_loss[i][-1])
          print("Final Training Accuracy:", train_acc[i][-1])
          print("Final Validation Accuracy:", valid_acc[i][-1])
  
    return w

alphas = [0.02, 0.002, 0.0002]
niters = [1000, 10000, 100000]

weights = []

for alpha in alphas:
  for niter in niters:
    weights.append([alpha, niter, solve_via_gradient_descent(alpha = alpha, X_train = X_train_norm, t_train=t_train, X_valid = X_valid_norm, t_valid = t_valid, niter = niter)])

for data in weights:

  alpha = data[0]
  niter = data[1]
  weight = data[2]

  weight = weight.tolist()

  # Add 0 weightes for missing information
  for i in range(3):
    weight[i].insert(15, 0)
    weight[i].insert(21, 0)
    weight[i].insert(27, 0)
    weight[i].insert(33, 0)
    weight[i].insert(39, 0)

  weight = np.array(weight)

  p = predt(weight, X_test_norm)

  temp = []
  for i in p:
    if i == 1:
      temp.append([1,0,0])
    elif i == 2:
      temp.append([0,1,0])
    elif i == 3:
      temp.append([0,0,1])

  p = np.array(temp)

  ratio = np.mean(p == t_test)

  # print("Weight: ")
  # print(weight.tolist())

  print("Alpha: ", alpha)
  print("niter: ", niter)
  print("Test accuracy: ", ratio)
  print("----------")

"""We chose Alpha = 0.0002 and niter = 100000.
Below is its corresponding trained weights.

Note: Since the traning set is randomized each time, the weights used in pred.py might be different the one presented below. However, both of the weights are procduced with Alpha = 0.0002 and niter = 100000.
"""

print("Mean (Col 3): ", mean_3)
print("Std (Col 3): ", std_3)
print("Mean (Col 4): ", mean_4)
print("Std (Col 4): ", std_4)
print("\n")

weight = weights[-1][-1]
weight = weight.tolist()

for i in range(3):
  weight[i].insert(15, 0)
  weight[i].insert(21, 0)
  weight[i].insert(27, 0)
  weight[i].insert(33, 0)
  weight[i].insert(39, 0)

print("Weight: ")
print(weight)